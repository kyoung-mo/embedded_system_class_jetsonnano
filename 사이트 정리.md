#사이트 정리

### 5.1 Test camera  

- **Onboard Camera (CSI, IMX219 지원)**  
  - `nvgstcapture-1.0`으로 실행  
  - `--prev-res`, `--cus-prev-res` 옵션으로 해상도 설정  
  - `j` 입력 시 이미지 저장, `q` 입력 시 종료  
  - 예제 스크립트: `python3 test_camera_csi.py`  

- **USB Camera**  
  - Ubuntu에서 드라이버 설치 불필요, `/dev/video*`로 인식  
  - `camorama`, `cheese` 앱 설치 후 실행 가능  
  - 예제 스크립트: `python3 test_camera_usb.py`  

---

### 5.2 Install JupyterLab and JetCam  

- **JupyterLab 소개 및 기능**  
  - 웹 기반 대시보드로 파일 브라우저, 노트북 실행, 터미널 접근 제공  
  - 코드 실행, 커널 관리, 셀 분리 뷰 등 다양한 기능 지원  
  - 브라우저에서 `IP:8888` 접속, 최초 로그인 시 비밀번호 필요  

- **설치 과정**  
  - Node.js & npm 설치 → `n` 모듈로 최신 버전 관리  
  - JupyterLab 설치: `pip3 install jupyter jupyterlab`  
  - 확장 설치: `jupyterlab-manager`, `statusbar`  
  - 설정 파일 생성 및 비밀번호 설정 → 서비스 등록 (`systemctl enable/start`)  

- **JetCam 설치**  
  - NVIDIA Jetson용 Python 카메라 인터페이스  
  - USB/CSI 카메라를 GStreamer 기반으로 numpy 배열 형태로 읽기 가능  
  - 설치:  
    ```bash
    git clone https://github.com/NVIDIA-AI-IOT/jetcam  
    cd jetcam  
    sudo python3 setup.py install
    ```  
  - 상세 사용법: [JetCam GitHub](https://github.com/NVIDIA-AI-IOT/jetcam)  

---

### 5.3 Install TensorFlow  

- **사전 준비**  
  - Jetson Orin Nano에는 Python 3.8 기본 설치됨  
  - pip 설치 및 최신 버전으로 업그레이드 필요 (`python3 -m pip install --upgrade pip`)  

- **머신러닝 필수 패키지 설치**  
  - `numpy`, `scipy`, `pandas`, `matplotlib`, `sklearn`  
  - 데이터 분석, 수치 계산, 시각화, 머신러닝 라이브러리 포함  

- **TensorFlow GPU 설치 전 확인**  
  - `nvcc -V` 명령어로 CUDA 설치 확인  
  - Python 의존성 패키지 설치 (`numpy==1.22`, `h5py==3.6.0`, `gast==0.4.0` 등)  

- **TensorFlow GPU 설치 방법**  
  - **온라인 설치**:  
    ```bash
    sudo pip3 install --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v51 tensorflow
    ```  
  - **오프라인 설치**: JetPack 버전에 맞는 `.whl` 파일 다운로드 후  
    ```bash
    pip3 install xxx.whl
    ```  

- **설치 확인**  
  - Python 실행 후 `import tensorflow as tf` 입력  
  - 오류가 없으면 성공적으로 설치 완료  

---

### 5.4 Install Torch & Torchvision  

- **Torch 설치**  
  - 제공된 `torch-xxx.whl` 파일을 Jetson Orin Nano에 업로드  
  - 설치:  
    ```bash
    pip3 install torch-xxx.whl
    ```  
  - pip3 기본 저장소의 Torch는 GPU 버전이 아닐 수 있으므로, 반드시 Jetson 공식 지원 버전 사용 필요  

- **Torchvision 설치**  
  - Torch 버전에 맞는 torchvision 버전 확인 (예: Torch 1.12 → Torchvision 0.13.0)  
  - 의존성 패키지 설치:  
    ```bash
    sudo apt-get install libjpeg-dev zlib1g-dev libpython3-dev libavcodec-dev libavformat-dev libswscale-dev
    ```  
  - 소스 빌드 및 설치:  
    ```bash
    git clone --branch v0.13.0 https://github.com/pytorch/vision torchvision
    cd torchvision
    export BUILD_VERSION=0.13.0
    python3 setup.py install --user
    ```  

- **설치 완료 확인**  
  - 설치 후 Python 실행 → `import torch, torchvision` 오류 없으면 정상 설치  

---

### 5.5 Jetson Reference Environment Construction  

- **사용 전 안내**  
  - Jetson Orin Nano 독자 이미지 구축용 튜토리얼  
  - YAHBOOM 이미지 사용 시 생략 가능  
  - 제공된 압축 패키지로도 빠른 환경 세팅 가능  

- **환경 구축 단계**  
  - 의존성 설치: `git`, `cmake`  
  - 소스코드 다운로드:  
    ```bash
    git clone https://github.com/dusty-nv/jetson-inference
    cd jetson-inference
    git submodule update --init
    ```  
  - Python 패키지 설치: `numpy`, `scipy`, `pandas`, `matplotlib`, `sklearn`  
  - Torch 휠 파일(`torch-1.8.0`) 업로드 후 설치  
  - `CMakePrebuild.sh` 수정 → `download-models.sh` 주석 처리  

- **모델 설치 방법**  
  - **온라인**: `./download-models.sh` 실행 → 자동 다운로드  
  - **오프라인**: `.tar.gz` 모델 파일 압축 해제  
    ```bash
    for tar in *.tar.gz; do tar xvf $tar; done
    ```  

- **빌드 및 설치**  
  ```bash
  cd jetson-inference
  mkdir build && cd build
  cmake ../
  make -j4
  sudo make install
  ```

- 설치 검증
  - cd jetson-inference/build/aarch64/bin./imagenet-console ./images/bird_0.jpg output.jpg
  - → 결과 이미지(output.jpg) 상단에 인식 결과 표시됨
 
---

### 5.6 Hello AI World  

- **TensorRT 소개**  
  - NVIDIA의 고성능 딥러닝 추론 플랫폼  
  - CPU 대비 최대 40배 빠른 추론 속도 제공  
  - CUDA 기반으로 INT8/FP16 최적화 지원  

- **주요 기능**  
  - 다양한 프레임워크에서 학습된 모델을 최적화 및 배포  
  - 영상 처리, 음성 인식, 추천 시스템, 자연어 처리 등 실시간 서비스에 활용  
  - 저지연, 고처리량 환경에서 적합  

- **Hello AI World 개요**  
  - Jetson 상에서 TensorRT 추론 및 PyTorch 전이학습 가능  
  - Python/C++ 기반 이미지 분류 및 객체 탐지 예제 제공  
  - 실시간 카메라 데모 실행 가능 (2시간 이내 완료)  
  - 전이학습은 장시간(하루 정도) 학습 권장  

---

### 5.7 Image Classification Inference  

- **ImageNet 기반 분류**  
  - GoogLeNet, ResNet-18 등 사전 학습된 분류 모델 사용  
  - 입력 이미지를 받아 1000개 카테고리에 대한 확률 출력  
  - C++/Python 샘플 코드 제공 → 결과 이미지를 저장  

- **다른 분류 모델 활용**  
  - `--network` 옵션으로 모델 지정 가능 (예: `--network=resnet-18`)  
  - Jetson 제공 이미지 사용 시 바로 실행 가능  
  - 독자 환경 구축 시에는 `resnet-18.tar.gz` 모델 다운로드 및 압축 해제 필요  

- **실시간 카메라 분류 데모**  
  - 지원 카메라: MIPI CSI(`csi://0`), V4L2(`/dev/video0`), RTSP 스트림  
  - 실행 시 실시간 스트림, 분류 객체 이름, 신뢰도, FPS 표시  
  - Jetson Orin Nano에서 GoogLeNet/ResNet-18 기준 약 75 FPS 성능 달성  

- **특징 요약**  
  - 최대 1000개 객체 분류 가능 (ImageNet ILSVRC 학습 데이터 기반)  
  - 클래스 이름 매핑은 `data/networks/ilsvrc12_synset_words.txt`에서 확인 가능  


---

### 5.8 Train Image Classification Models  

- **사전 준비**  
  - Torch(CPU+GPU) 설치 필요 (`./install-pytorch.sh`)  
  - Yahboom 이미지에는 Torch 기본 설치됨  
  - 데이터셋 준비: 고양이/강아지 이미지(`cat_dog.tar.gz`) 다운로드 및 압축 해제  

- **모델 학습 실행**  
  - 기본 명령어:  
    ```bash
    python3 train.py --model-dir=models/cat_dog data/cat_dog
    ```  
  - 옵션: `--epochs=N` (학습 횟수), `--batch=N` (배치 크기)  
  - 메모리 부족 시 swap 설정 및 GUI 비활성화 필요  

- **학습 과정 해석**  
  - `Loss`: 모델 예측과 정답의 오차  
  - `Acc@1`, `Acc@5`: 정확도 지표  
  - ResNet-18 기준 → 약 30 epoch: 80% 정확도, 65 epoch: 82.5% 정확도 도달  
  - 기본 설정: 35 epoch (~55분 소요)  

- **모델 변환 및 TensorRT 추론**  
  - 학습 완료 후 ONNX/ONnano 변환:  
    ```bash
    python3 onnano_export.py --model-dir=models/cat_dog
    ```  
  - TensorRT 추론 실행 예시:  
    ```bash
    imagenet.py --model=models/cat_dog/resnet18.onnano \
                --input_blob=input_0 --output_blob=output_0 \
                --labels=data/cat_dog/labels.txt data/cat_dog/test/cat/01.jpg cat.jpg
    ```  
  - 카메라 입력도 지원 (`csi://0`, `v4l2:///dev/video0`)  

- **학습 결과 비교**  
  - 1 epoch → 낮은 정확도  
  - 35 epoch → 약 80% 정확도  
  - 100 epoch → 정확도 크게 향상됨  

- **추가 참고**  
  - 다른 이미지 데이터셋 학습 가이드:  
    [pytorch-collect.md](https://github.com/dusty-nv/jetson-inference/blob/master/docs/pytorch-collect.md)  

---

### 5.9 Target Detection Inference  

- **DetectNet 개요**  
  - 입력 이미지 → 객체의 **바운딩 박스 좌표, 클래스, 신뢰도** 출력  
  - 기본 모델: **SSD-Mobilenet-v2 (MS COCO, 91 클래스)**  
  - Python / C++ 모두 지원, TensorRT로 실시간 추론 가능  

- **이미지 객체 탐지**  
  - 실행 예시:  
    ```bash
    ./detectnet --network=ssd-mobilenet-v2 images/xingren.png images/test/output_xingren.png
    ./detectnet.py --network=ssd-mobilenet-v2 images/xingren.png images/test/output_xingren.png
    ```  
  - 옵션:  
    - `--overlay`: box, line, labels, conf 설정  
    - `--alpha`: 바운딩 박스 투명도 조절  
    - `--threshold`: 탐지 민감도 조절 (기본 0.5)  

- **비디오 파일 처리**  
  - 실행 예시:  
    ```bash
    ./detectnet pedestrians.mp4 images/test/pedestrians_ssd.mp4
    ./detectnet.py pedestrians.mp4 images/test/pedestrians_ssd.mp4
    ```  

- **실시간 카메라 객체 탐지**  
  - 지원 입력:  
    - MIPI CSI: `csi://0`  
    - V4L2: `/dev/video0`  
    - RTSP: `rtsp://username:password@ip:port`  
  - 실행 예시:  
    ```bash
    ./detectnet csi://0
    ./detectnet /dev/video0
    ./detectnet.py /dev/video0 output.mp4
    ```  
  - 결과: OpenGL 창에서 실시간 스트림 + 바운딩 박스 + FPS 표시  

- **추가 팁**  
  - 처음 실행 시 TensorRT 최적화 캐시 생성 → 이후 실행 속도 향상  
  - 원하는 객체가 탐지되지 않거나 오탐 발생 시 `--threshold` 값 조정  


---

### 5.10 Training Object Detection Model  

- **사전 준비**  
  - Torch(CPU+GPU) 설치 필요 (`./install-pytorch.sh`)  
  - Yahboom 이미지에는 Torch 및 기본 모델 포함 → 생략 가능  
  - 완전한 scratch 학습 대신 **사전학습 모델 재학습** 방식 사용  

- **기본 모델 & 데이터셋 준비**  
  - SSD MobileNet v1 기본 모델 다운로드 (`mobilenet-v1-ssd-mp-0_675.pth`)  
  - 과일 이미지 데이터셋 다운로드 (`open_images_downloader.py`)  
    - 옵션: `--max-images`, `--max-annotations-per-class` 로 데이터 크기 제한 가능  
  - Yahboom 이미지에는 이미 데이터 포함  

- **모델 학습 실행**  
  - 예시 명령어:  
    ```bash
    python3 train_ssd.py --data=data/fruit --model-dir=models/fruit --batch-size=4 --epochs=30
    ```  
  - 파라미터 설정:  
    - `--batch-size` (기본=4), `--epochs` (기본=30, 권장=100)  
    - `--resume` 옵션으로 중단 지점부터 재개 가능  
  - 메모리 부족 시 swap 활성화 & GUI 비활성화 필요  

- **모델 변환 및 추론**  
  - 학습 완료 후 TensorRT용 ONnano 변환:  
    ```bash
    python3 onnano_export.py --model-dir=models/fruit
    ```  
  - 이미지 추론:  
    ```bash
    detectnet --model=models/fruit/ssd-mobilenet.onnano \
              --labels=models/fruit/labels.txt \
              --input-blob=input_0 --output-cvg=scores --output-bbox=boxes \
              data/fruit/fruit_*.jpg data/fruit/test/fruit_%i.jpg
    ```  
  - 카메라 추론:  
    ```bash
    detectnet --model=models/fruit/ssd-mobilenet.onnano --labels=models/fruit/labels.txt \
              --input-blob=input_0 --output-cvg=scores --output-bbox=boxes csi://0
    detectnet --model=models/fruit/ssd-mobilenet.onnano --labels=models/fruit/labels.txt \
              --input-blob=input_0 --output-cvg=scores --output-bbox=boxes v4l2:///dev/video0
    ```  

- **학습 결과**  
  - 30 epoch 결과: 정확도 낮음, 일부 이미지 인식 불가  
  - 100 epoch 결과: 정확도 크게 향상, 안정적인 탐지 성능 확보  
  - Yahboom 이미지에는 이미 30회/100회 학습된 모델 제공  


---

### 5.11 Semantic Segmentation  

- **개요**  
  - **Semantic Segmentation**: 이미지 전체가 아닌 **픽셀 단위 분류** 수행  
  - FCN(완전 합성곱 네트워크) 기반 → 각 픽셀에 클래스 레이블 부여  
  - 대표 네트워크: **FCN-ResNet18**, SegNet (Python/C++ 지원)  

- **이미지 분할 예시**  
  - 도시 장면 (Cityscapes 데이터셋)  
    ```bash
    ./segnet --network=fcn-resnet18-cityscapes images/city_0.jpg images/test/output.jpg
    ./segnet.py --network=fcn-resnet18-cityscapes images/city_0.jpg images/test/output.jpg
    ```  
  - 오프로드 환경 (DeepScene 데이터셋)  
    ```bash
    ./segnet --network=fcn-resnet18-deepscene images/trail_0.jpg images/test/output_overlay.jpg
    ./segnet.py --network=fcn-resnet18-deepscene images/trail_0.jpg images/test/output_overlay.jpg
    ```  
  - `--visualize=mask` 옵션으로 분할 마스크만 출력 가능  

- **실시간 카메라 분할 데모**  
  - 입력 소스:  
    - MIPI CSI (`csi://0`)  
    - V4L2 (`/dev/video0`)  
    - RTSP (`rtsp://username:password@ip:port`)  
  - 실행 예시:  
    ```bash
    ./segnet --network=fcn-resnet18-deepscene csi://0
    ./segnet.py --network=fcn-resnet18-deepscene /dev/video0
    ```  
  - 결과: OpenGL 창에 실시간 스트림 + 분할 마스크 오버레이 출력  

- **지원 모델**  
  - FCN-ResNet18 (Cityscapes, DeepScene, MHP 등 다양한 데이터셋 학습)  
  - 모델은 [jetson-inference GitHub](https://github.com/dusty-nv/jetson-inference)에서 다운로드 가능  

---

### 5.12 Action Recognition  

- **개요**  
  - **Action Recognition**: 비디오 프레임 시퀀스에서 활동/행동/제스처 분류  
  - ResNet18 기반 모델은 **16 프레임 윈도우**를 사용하며, 프레임을 건너뛰어 시간 창 확장 가능  
  - `actionNet` 객체: 프레임 버퍼링 후 가장 높은 확률의 행동 클래스 출력  
  - Python / C++ 모두 지원  

- **실행 예시**  
  - **카메라 입력 (V4L2)**  
    ```bash
    ./actionnet /dev/video0
    ./actionnet.py /dev/video0
    ```  
  - **비디오 파일 입력/출력**  
    ```bash
    ./actionnet input.mp4 output.mp4
    ./actionnet.py input.mp4 output.mp4
    ```  
  - 지원 형식: mp4, mkv, avi, flv  

- **모델 특징**  
  - 기본 모델: **resnet18 기반 ActionNet**  
  - 학습 데이터셋: **Kinetics-700**, **Moments in Time**  
  - 카테고리 레이블 리스트 포함 (행동/활동 분류 기준)  

- **주의사항**  
  - 독자 환경 구축 시 모델 파일을 network 폴더에 다운로드 필요  
  - Yahboom 이미지 사용 시 기본 모델 포함되어 있어 바로 실행 가능  

---

### 5.13 Pose Estimation  

- **개요**  
  - **Pose Estimation**: 인체의 주요 관절(Keypoints) 위치를 찾아 뼈대(links)로 연결  
  - 활용 분야: 제스처 인식, AR/VR, HMI, 자세/보행 교정 등  
  - 제공 모델: **ResNet18 기반 Body Pose**, **ResNet18 기반 Hand Pose**  
  - Python / C++ 지원, 멀티 인원 추적 가능  

- **이미지 포즈 추정**  
  - 실행 예시:  
    ```bash
    ./posenet images/humans_4.jpg images/test/pose_humans_%i.jpg
    ./posenet.py images/humans_4.jpg images/test/pose_humans_%i.jpg
    ```  
  - 결과 이미지에 인체 관절 위치 및 연결선 출력  

- **비디오/실시간 카메라 추정**  
  - V4L2 또는 CSI 카메라 입력 지원 (`/dev/video0`, `csi://0`)  
  - 실행 예시:  
    ```bash
    ./posenet /dev/video0
    ./posenet.py /dev/video0
    ```  
  - **손 관절 추정** 옵션:  
    ```bash
    ./posenet --network=resnet18-hand /dev/video0
    ./posenet.py --network=resnet18-hand /dev/video0
    ```  

- **주의사항**  
  - 독자 환경 구축 시 모델 파일을 `networks` 폴더에 다운로드 필요  
  - Yahboom 이미지에는 기본 모델 포함되어 있어 즉시 실행 가능  


---

### 5.14 Background Removal  

- **개요**  
  - **Background Removal (배경 제거/분리)**: 이미지에서 전경과 배경을 분리하는 마스크 생성  
  - 활용: 배경 교체/블러 처리, 화상 회의, 객체 탐지/추적, 모션 감지 등  
  - 사용 모델: **U²-Net (Fully Convolutional Network 기반)**  
  - Python / C++ 모두 지원  

- **이미지 배경 제거/교체 예시**  
  ```bash
  # C++
  ./backgroundnet images/bird_0.jpg images/test/bird_mask.png
  ./backgroundnet --replace=images/snow.jpg images/bird_0.jpg images/test/bird_replace.jpg

  # Python
  ./backgroundnet.py images/bird_0.jpg images/test/bird_mask.png
  ./backgroundnet.py --replace=images/snow.jpg images/bird_0.jpg images/test/bird_replace.jpg
  ```
- replace 옵션: 지정한 이미지를 배경으로 교체 (자동 리사이즈 적용)  

- **실시간 카메라 배경 제거/교체**  
  ```bash
  # C++
  ./backgroundnet /dev/video0
  ./backgroundnet --replace=images/coral.jpg /dev/video0

  # Python
  ./backgroundnet.py /dev/video0
  ./backgroundnet.py --replace=images/coral.jpg /dev/video0
  ```
- 출력: 모니터 표시(기본값), 네트워크 전송(WebRTC 유사), 영상 파일 저장 가능

---

### 5.15 Monocular Depth Estimation  

- **개요**  
  - 단안 카메라 입력만으로 **상대적 깊이(Depth Map)** 추정 가능  
  - 기존에는 스테레오/ RGB-D 카메라 필요 → DNN 기반으로 단일 영상에서도 추론 가능  
  - 대표 연구: MIT **FastDepth** (FCN 기반)  
  - Python / C++ 모두 지원  

- **이미지 단안 깊이 추정**  
  - 출력: 컬러 Depth Map (시각화용), 원본 깊이 필드 접근 가능  

- **비디오/실시간 카메라 추정**  
  - 입력 소스: MIPI CSI 카메라(`csi://0`), V4L2 카메라(`/dev/video0`)  
  - 옵션:  
    - `--depth-scale=0.5` → 출력 깊이 맵 크기 축소  
    - `--input-width=X --input-height=Y` → 입력 해상도 조정  

- **특징 요약**  
  - 실시간 추론 시 TensorRT 최적화 캐시 생성 → 이후 실행 속도 향상  
  - 실내/실외 장면 모두 적용 가능 (장애물 감지, 내비게이션, 지도 작성 등)  

---

### 5.16 DeepStream Construction  

- **개요**  
  - Jetson Orin Nano에서 **DeepStream SDK 6.2** 설치 및 구성 튜토리얼  
  - YAHBOOM 이미지 사용자는 이미 설치되어 있으므로 생략 가능  

- **환경 준비**  
  - NVIDIA 공식 사이트에서 DeepStream 6.2 버전 다운로드  
  - 사전 의존성 패키지 설치 필요 (`libssl1.1`, `gstreamer1.0-*`, `libyaml-cpp-dev` 등)  

- **설치 과정**  
  - **librdkafka 설치**  
    - GitHub에서 소스코드 클론 후 빌드 및 설치  
    - `/opt/nvidia/deepstream/deepstream-6.2/lib` 경로에 복사  
  - **DeepStream 설치**  
    - `deepstream_sdk_v6.2.0_jetson.tbz2` 다운로드 및 Jetson에 업로드  
    - 압축 해제 후 설치 스크립트 실행 (`install.sh`)  
    - `sudo ldconfig` 실행하여 라이브러리 등록  

- **검증**  
  - 설치 완료 후 버전 확인:  
    ```bash
    deepstream-app --version-all
    ```  

- **참고 자료**  
  - [NVIDIA DeepStream Quickstart Guide](https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Quickstart.html#update-bsp-library)  

---

### 5.17 Vehicle Inspection  

- **개요**  
  - DeepStream SDK를 활용한 **Back-to-Back Detector Application**  
  - `deepstream-app` 기반 예제이며, 여러 객체 검출기를 파이프라인에 순차적으로 적용  
  - 1차 검출기: 사람/차량/자전거/도로 표지판 → 2차 검출기로 결과 전달  

- **모델 다운로드**  
  - GitHub에서 차량 검출 모델(caffe 기반) 다운로드:  
    - `fd_lpd.caffemodel`, `fd_lpd.prototxt`, `labels.txt`  
  - 기본 경로: `/opt/nvidia/deepstream/deepstream/samples/models/Secondary_FaceDetect`  
  - 다운로드 불가 시, PC에서 받아서 Jetson Orin Nano로 전송  

- **컴파일 및 실행**  
  1. 소스 경로 이동:  
     ```bash
     cd /opt/nvidia/deepstream/deepstream/sources/apps/sample_apps/deepstream_reference_apps-master/back-to-back-detectors
     ```  
  2. `Makefile` 수정 → `CUDA_VER` Jetson 기준 `11.4` 설정  
  3. 빌드 및 실행:  
     ```bash
     sudo make
     ./back-to-back-detectors ../../../../../samples/streams/sample_720p.h264
     ```  
  4. `secondary_detector_config.txt` 파일 내 모델 경로를 다운로드한 모델 위치로 수정  

- **특징 요약**  
  - 2개의 `nvinfer` 요소가 **nvstreammux → display** 사이에 삽입됨  
  - 각 `nvinfer`는 독립적인 설정 파일 사용  
  - 기본 제공 예제 `deepstream-test1` 기반으로 확장된 구조  

---

### 5.18 Attitude Detection  

- **개요**  
  - DeepStream SDK 기반의 **3D 인체 자세 추정(Body Pose 3D)** 애플리케이션  
  - 사람의 3D 관절(Keypoints)을 추정하여 JSON/영상으로 출력  
  - 활용: HMI, AR/VR, 헬스케어, 동작 분석  

- **환경 준비**  
  - DeepStream SDK 6.2 설치 필수  
  - 경로 설정:  
    ```bash
    export BODYPOSE3D_HOME=/opt/nvidia/deepstream/deepstream/sources/apps/sample_apps/deepstream_reference_apps/deepstream-bodypose-3d
    ```  
  - 모델 다운로드 (NGC CLI):  
    - **PeopleNet**  
    - **BodyPose3DNet**  
  - Eigen 3.4.0 라이브러리 설치 후 심볼릭 링크 연결  

- **빌드 과정**  
  - 커스텀 nvinfer 파서 빌드  
  - deepstream-pose-estimation-app 빌드 (`CUDA_VER=11.4`)  
  - 빌드 완료 후 실행 파일 생성 확인  

- **실행 및 결과**  
  - 비디오 입력 실행:  
    ```bash
    ./deepstream-pose-estimation-app --input file://$BODYPOSE3D_HOME/streams/bodypose.mp4
    ```  
  - 출력: 실시간 스켈레톤 오버레이 (최대 30FPS)  
  - 결과 저장 옵션:  
    - 영상: `$BODYPOSE3D_HOME/streams/bodypose_3dbp.mp4`  
    - 키포인트 JSON: `$BODYPOSE3D_HOME/streams/bodypose_3dbp.json`  

- **출력 데이터 형식**  
  - **Pose25d**: `[x, y, zRel, conf]` (상대 깊이, 이미지 좌표계, mm 단위)  
  - **Pose3d**: `[x, y, z, conf]` (절대 위치, 월드 좌표계, mm 단위)  

- **네트워크 출력**  
  - RTSP 스트림 출력: `rtsp://localhost:8554/ds-test`  
  - 메시지 브로커 발행:  
    ```bash
    ./deepstream-pose-estimation-app --input file://... --conn-str "localhost;9092;test"
    ```  
    - `localhost`: 브로커 주소  
    - `9092`: 포트  
    - `test`: 토픽명  

---

### 5.19 Introduction to YOLOv5  

- **YOLO란?**  
  - "You Only Look Once"의 약자로, 이미지를 격자(Grid)로 분할하여 객체의 중심이 속한 셀에서 탐지를 수행하는 객체 탐지 알고리즘  
  - 빠른 속도와 높은 정확도로 가장 널리 쓰이는 Object Detection 모델 중 하나  

- **YOLOv5 개요**  
  - Ultralytics에서 개발한 **PyTorch 기반 오픈소스 버전**  
  - YOLOv4 대비 향상된 실시간 성능과 정확도 제공  
  - 다양한 모델 크기(소형~대형)로 경량 디바이스부터 고성능 GPU까지 폭넓게 활용 가능  

- **성능 특징**  
  - **빠른 추론 속도**  
    - CPU: 이미지당 약 7ms → 140 FPS (YOLOv4 대비 3배 수준)  
    - GPU: 최대 400 FPS 이상 달성 가능  
  - **작은 모델 크기**  
    - YOLOv5 가중치 파일 크기는 YOLOv4 대비 약 **1/9 수준**  
  - **짧은 학습 시간**  
    - COCO 2017 데이터셋 기준, V100 GPU 1개로 학습 시 더 빠른 훈련 속도 확보  

- **장점 요약**  
  - 초고속 추론 성능 (실시간 객체 탐지에 적합)  
  - 메모리 사용량 절감 (임베디드 환경 친화적)  
  - 짧은 학습 주기와 PyTorch 기반의 높은 활용성  

---

### 5.20 YOLOv5 Construction  

- **개요**  
  - Jetson Orin Nano에서 YOLOv5 환경을 직접 구축하는 튜토리얼  
  - YAHBOOM 버전 이미지 사용자는 이미 환경이 포함되어 있으므로 생략 가능  

- **환경 준비**  
  - 필수 패키지 설치:  
    ```bash
    sudo apt-get install python3-pip libopenblas-base libopenmpi-dev
    pip3 install Cython numpy
    ```  
  - Torch 설치 (사전 빌드된 `.whl` 파일 사용, WinSCP로 전송)  
    ```bash
    pip3 install torch-1.12.0a0+2c916ef.nv22.3-cp38-cp38-linux_aarch64
    ```  
  - 의존성 패키지 설치:  
    ```bash
    sudo apt-get install libjpeg-dev zlib1g-dev libpython3-dev libavcodec-dev libavformat-dev libswscale-dev
    ```  

- **Torchvision 설치**  
  - YOLOv5와 호환되는 버전(예: v0.13.0) 설치:  
    ```bash
    git clone --branch v0.13.0 https://github.com/pytorch/vision torchvision
    cd torchvision
    export BUILD_VERSION=0.13.0
    python3 setup.py install --user
    ```  

- **YOLOv5 소스코드 다운로드 및 설정**  
  - YOLOv5 소스코드 클론:  
    ```bash
    git clone https://github.com/marcoslucianops/DeepStream-Yolo
    cd yolov5
    ```  
  - pip 최신화 및 requirements 설치:  
    ```bash
    python3 -m pip install --upgrade pip
    pip3 install -r requirements.txt -i https://mirror.baidu.com/pypi/simple
    ```  
  - Jetson Orin Nano에는 OpenCV 4.5.4 기본 포함 → `requirements.txt`의 `opencv` 항목에 `#` 주석 처리 필요  

- **빌드 검증**  
  - YOLOv5 동작 확인:  
    ```bash
    cd ~/yolov5
    python3 detect.py
    ```  
  - 첫 실행 시 `yolov5s.pt` 가중치 자동 다운로드 (네트워크 불가 시 수동 업로드)  
  - 결과 저장 경로: `yolov5/runs/detect/exp*`  

- **에러 수정 (torch 버전 관련)**  
  - `upsampling.py` 수정 필요 (라인 153):  
    ```bash
    sudo gedit /usr/local/lib/python3.8/dist-packages/torch/nn/modules/upsampling.py
    ```  
  - `recompute_scale_factor=self.recompute_scale_factor` 앞에 `#` 주석 추가 후 저장  
  - 이후 `python3 detect.py` 재실행 → 정상 동작 확인 가능  

---

### 5.21 YOLOv5 Real-Time Detection  

- **사용 방법**  
  - YAHBOOM 버전 이미지 + USB 카메라 사용 시 `datasets.py` 수정 필요  
    - `~/yolov5/utils/datasets.py`  
    - 292번 줄 주석 해제, 293번 줄에 `#` 추가  
  - 실행 명령어:  
    ```bash
    cd ~/yolov5 && python3 detect.py --source 0
    ```  
  - 실행 후 CSI 카메라가 켜지고, 화면에 객체 탐지 결과가 실시간 표시됨  
  - 종료: `Ctrl + C`  
  - 결과 저장 경로: `yolov5/runs/detect/exp` (비디오 파일)  

- **주의사항**  
  - 네트워크 오류 발생 시 `yolov5s.pt` 가중치를 수동으로 `yolov5` 폴더에 추가  
  - CSI 카메라는 **Jetson Orin NX 16G** 및 해당 시스템에서만 정상 동작  
    - 다른 보드에서는 JetPack 버전/성능 문제로 동작 불가  
  - 직접 빌드한 이미지일 경우 `datasets.py`를 수동 수정해야 함  
    - [참고 링크 1](https://blog.csdn.net/AlwaysNoError/article/details/123298884)  
    - [참고 링크 2](https://blog.csdn.net/m0_50004939/article/details/126739291) (SPPF 오류 해결)
   
---

### 5.22 YOLOv5 + TensorRT Acceleration  

- **사용 전 주의사항**  
  - YAHBOOM 버전 이미지는 이미 환경이 구축되어 있어 별도 설정 불필요  
  - 직접 빌드 시 TensorRT 패키지 설치 필요  
  - YOLOv5 버전과 TensorRT 가속 패키지 버전이 일치해야 함 (본 튜토리얼: YOLOv5 v5.0 + TensorRT v5.0)  
  - TensorRT YOLOv5 패키지: [tensorrtx/yolov5](https://github.com/wang-xinyu/tensorrtx/tree/master/yolov5)  

- **TensorRT 변환 과정**  
  1. `gen_wts.py` 복사 후 `.wts` 파일 생성  
     ```bash
     python3 gen_wts.py yolov5s.pt
     ```  
  2. `tensorrtx/yolov5` 폴더에서 빌드  
     ```bash
     mkdir build && cd build
     cmake ..
     make -j4
     ```  
  3. 필요 시 `yololayer.h`의 `CLASS_NUM` 수정 (COCO 기본값 = 80)  
  4. `.wts` 파일을 `tensorrtx/yolov5`에 복사 후 `.engine` 파일 생성  
     ```bash
     sudo ./yolov5 -s ../yolov5s.wts yolov5s.engine s
     ```  

- **테스트 및 성능 검증**  
  - **방법 1: C++ 실행**  
    ```bash
    cd tensorrtx/yolov5/build
    sudo ./yolov5 -d yolov5s.engine ../samples
    ```  
  - **방법 2: Python 실행**  
    ```bash
    cd tensorrtx/yolov5
    python3 yolov5_trt.py
    ```  
  - 두 경우 모두 YOLOv5 기본 실행(`python3 detect.py`)과 비교 시 **TensorRT 적용 후 추론 속도가 크게 향상**됨  

- **특징 요약**  
  - TensorRT 적용 시 이미지/영상 처리 시간 대폭 단축  
  - 실시간 객체 탐지 환경에서 FPS 향상 및 지연(latency) 감소 확인 가능  

---

### 5.23 YOLOv5 + TensorRT Acceleration + DeepStream  

- **사용 전 주의사항**  
  - YAHBOOM 버전 이미지는 DeepStream 환경이 포함되어 있어 별도 설정 불필요  
  - 직접 빌드한 경우, DeepStream 환경을 먼저 설치해야 함 (참고: DeepStream 구축 튜토리얼)  

- **설치 및 설정 과정**  
  1. **모델 변환**  
     - YOLOv5 모델을 TensorRT용 `.wts` 파일로 변환  
       ```bash
       git clone https://github.com/marcoslucianops/DeepStream-Yolo.git
       cd DeepStream-Yolo/utils
       cp gen_wts_yoloV5.py ../../yolov5
       cd ../../yolov5
       python3 gen_wts_yoloV5.py -w yolov5s.pt
       ```  
     - 변환 후 `yolov5n.cfg`, `yolov5n.wts` 파일 생성  

  2. **모델 배치**  
     - 변환된 `yolov5n.cfg`, `yolov5n.wts` 파일을 Jetson Orin Nano의 DeepStream-Yolo 디렉토리에 복사  

  3. **DeepStream 설정 수정** (YAHBOOM 이미지 사용 시 생략 가능)  
     - `deepstream_app_config.txt`에 YOLOv5 설정 추가:  
       ```txt
       config-file=config_infer_primary_yoloV5.txt
       ```  
     - `config_infer_primary_yoloV5.txt` 수정:  
       ```txt
       model-engine-file=model_b2_gpu0_fp16.engine
       batch-size=2
       network-mode=2   # 2 = FP16 강제 사용
       ```  
     - FPS 최적화: 입력 크기, batch size, interval 등 조정 가능  

- **컴파일 및 실행**  
  ```bash
  cd nvdsinfer_custom_impl_Yolo/
  CUDA_VER=11.4 make -j4
  cd ..
  deepstream-app -c deepstream_app_config.txt
  ```

- 실행 후 CSI 카메라 화면 출력 확인 가능  
- USB 카메라 사용 시 `deepstream_app_config_usb.txt`로 실행  

- **특징 요약**  
  - YOLOv5 + TensorRT + DeepStream 조합으로 **최대 FPS 향상** 및 **지연 시간 최소화**  
  - **실시간 객체 탐지 및 스트리밍 환경**에 최적화  
  - 다양한 입력 소스 지원 (CSI 카메라, USB 카메라, RTSP 스트림 등)  

- **참고 링크**  
  - [DeepStream 설정 파일 설명](https://blog.csdn.net/weixin_38369492/article/details/104859567) 

---

### 5.24 Mediapipe Environment  

- **사용 전 주의사항**  
  - 이 튜토리얼은 **직접 구축한 이미지** 환경에 해당  
  - YAHBOOM 버전 이미지 사용자는 이미 환경 포함 → 생략 가능  

- **환경 구축 과정**  
  1. **파일 준비**  
     - 첨부된 `bazel`, `mediapipe-0.8.4-cp38-cp38-linux_aarch64.whl` 파일을 Jetson Orin Nano로 전송  
  2. **Bazel 설치**  
     ```bash
     sudo chmod +x bazel
     mv bazel /usr/local/bin
     bazel --version   # 설치 확인
     ```  
  3. **Mediapipe 설치**  
     ```bash
     pip3 install opencv-contrib-python==3.4.17.63
     pip3 install mediapipe-0.8.4-cp38-cp38-linux_aarch64.whl
     pip3 uninstall opencv-contrib-python
     ```  
  4. **설치 검증**  
     ```python
     python3
     import mediapipe as mp
     ```  
     - 오류가 없으면 설치 성공  

- **참고 자료**  
  - [Mediapipe 설치 튜토리얼](https://blog.csdn.net/weixin_43659725/article/details/120211312)  


---

### 5.25 MediaPipe Development  

- **개요**  
  - Google에서 개발한 **MediaPipe**는 데이터 스트림 처리 기반의 머신러닝 애플리케이션 프레임워크  
  - 비디오, 오디오, 센서 데이터 등 다양한 시계열 데이터를 처리할 수 있으며, **멀티 플랫폼 지원** (임베디드, 모바일, 데스크톱, 서버)  
  - 핵심 개념: **Packets, Streams, Calculators, Graphs, Subgraphs**  
  - 주요 특징:  
    - End-to-End 가속 (일반 하드웨어에서도 ML 추론 가속)  
    - Cross-platform (Android, iOS, Web, IoT 포함)  
    - 다양한 ML 솔루션 제공 (Face, Hands, Pose 등)  
    - Apache 2.0 오픈소스, 커스터마이징 가능  

- **주요 솔루션**  
  - 얼굴 인식: Face Detection, Face Mesh, Iris  
  - 신체 인식: Hands, Pose, Holistic, Hair Segmentation  
  - 객체/동작: Object Detection, Box Tracking, Instant Motion Tracking, Objectron, KNIFT  
  - 지원 언어 및 플랫폼: Android, iOS, C++, Python, JS, Coral  

- **MediaPipe Hands**  
  - 손과 손가락 21개 관절을 **3D 좌표**로 추적  
  - Palm Detection 후 회귀 기반 좌표 추정  
  - 실제 데이터(약 3만 장)와 합성 데이터를 통해 학습 → **부분 가림, 자가 가림에도 강건**  

- **MediaPipe Pose**  
  - **BlazePose 기반** 신체 자세 추적  
  - RGB 프레임으로부터 **33개 3D 좌표** + **배경 세그멘테이션 마스크** 생성  
  - ML Kit Pose Detection API에 활용  

- **dlib**  
  - C++ 기반 머신러닝 라이브러리, **얼굴 특징점(68 포인트) 검출**에 널리 사용  
  - 응용: 얼굴 인식, 로보틱스, 임베디드, 고성능 컴퓨팅  
  - 예: `shape_predictor_68_face_landmarks.dat` 모델을 이용한 얼굴 특징 추정  

- **관련 링크**  
  - [MediaPipe GitHub](https://github.com/google/mediapipe)  
  - [MediaPipe 공식 문서](https://google.github.io/mediapipe/)  
  - [dlib 공식 홈페이지](http://dlib.net/)  
  - [dlib GitHub](https://github.com/davisking/dlib)  

